{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0b49949-bb27-4dd0-9ceb-5a97e98faec9",
   "metadata": {},
   "source": [
    "# Feature and Object Tracking in Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b69a33a-2f93-433e-896b-02dea7b805c6",
   "metadata": {},
   "source": [
    "## 1. Introduction and Objectives\n",
    "\n",
    "This tutorial explores feature tracking and object tracking, two fundamental techniques in computer vision that enable systems to follow entities across video frames. By the end of this tutorial, you will:\n",
    "\n",
    "- Understand the difference between detection and tracking\n",
    "- Distinguish between feature tracking and object tracking\n",
    "- Comprehend single vs. multi-object tracking paradigms\n",
    "- Implement the Kanade-Lucas-Tomasi (KLT) tracker\n",
    "- Apply these concepts to solve real-world tracking problems\n",
    "\n",
    "Tracking is essential in numerous applications including surveillance, autonomous vehicles, human-computer interaction, and augmented reality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aa71ea-77fa-4a6c-afe9-fad65e9a4899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63ff5e56-56c4-44bf-b87b-fe4c2ded561e",
   "metadata": {},
   "source": [
    "## 2. Theoretical Foundations\n",
    "\n",
    "### 2.1 Detection vs. Tracking\n",
    "\n",
    "**Detection** identifies objects in a single frame without utilizing temporal information:\n",
    "- Operates on individual frames independently\n",
    "- Answers \"what\" and \"where\" in a single frame\n",
    "- Examples: YOLO, SSD, Faster R-CNN\n",
    "\n",
    "**Tracking** follows objects across multiple frames by exploiting temporal consistency:\n",
    "- Maintains identity across time\n",
    "- Leverages motion information\n",
    "- More computationally efficient than performing detection on every frame\n",
    "- Examples: KLT, SORT, DeepSORT\n",
    "\n",
    "### 2.2 Feature Tracking vs. Object Tracking\n",
    "\n",
    "**Feature Tracking** follows distinctive points or patterns:\n",
    "- Tracks low-level features (corners, edges, distinctive points)\n",
    "- Typically uses local appearance and motion models\n",
    "- Focuses on \"how\" individual parts move\n",
    "\n",
    "**Object Tracking** follows entire objects:\n",
    "- Tracks high-level semantic entities (people, cars, etc.)\n",
    "- Often combines detection and tracking\n",
    "- Focuses on \"what\" is being tracked and \"where\" it is\n",
    "\n",
    "### 2.3 Single Object vs. Multi-Object Tracking\n",
    "\n",
    "**Single Object Tracking (SOT)**:\n",
    "- Focuses on one target defined in the first frame\n",
    "- Doesn't handle object initialization/termination\n",
    "- Examples: KCF, MOSSE, SiamFC\n",
    "\n",
    "**Multiple Object Tracking (MOT)**:\n",
    "- Follows multiple objects simultaneously\n",
    "- Handles object appearance/disappearance\n",
    "- Requires data association to maintain identities\n",
    "- Examples: SORT, DeepSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a8dc3d-667a-41cd-8aed-e1b9c9c8d43b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2ab20b4-659c-49a3-8d71-d437f1224f0f",
   "metadata": {},
   "source": [
    "### 2.4 KLT Tracker - Mathematical Foundations\n",
    "\n",
    "The Kanade-Lucas-Tomasi (KLT) tracker is a feature-based method that tracks points across frames by analyzing optical flow. It is based on these key assumptions:\n",
    "\n",
    "1. **Brightness constancy**: The intensity of a point remains consistent between frames\n",
    "   \n",
    "   $I(x,y,t) = I(x+\\Delta x, y+\\Delta y, t+\\Delta t)$\n",
    "\n",
    "2. **Small motion**: The movement between frames is small, allowing for linearization\n",
    "   \n",
    "   $I(x+\\Delta x, y+\\Delta y, t+\\Delta t) \\approx I(x,y,t) + \\frac{\\partial I}{\\partial x}\\Delta x + \\frac{\\partial I}{\\partial y}\\Delta y + \\frac{\\partial I}{\\partial t}\\Delta t$\n",
    "\n",
    "3. **Spatial coherence**: Neighboring points belong to the same surface and move together\n",
    "\n",
    "The KLT algorithm minimizes the sum of squared differences (SSD) between feature patches in consecutive frames:\n",
    "\n",
    "$\\min_{\\Delta x, \\Delta y} \\sum_{(x,y) \\in W} [I(x+\\Delta x, y+\\Delta y, t+\\Delta t) - I(x,y,t)]^2$\n",
    "\n",
    "Where $W$ is the window around the feature point.\n",
    "\n",
    "## 3. Implementation - Feature and Object Tracking\n",
    "\n",
    "Let's implement and explore these tracking concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728c0ba3-4165-411d-902e-bc3dd0a2e2db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef3daab9-8878-4340-81d0-56761fa86069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a6c7b0-ed9e-455b-af72-85b7c9df8fd4",
   "metadata": {},
   "source": [
    "### 3.2 Feature Selection - Good Features to Track\n",
    "\n",
    "Before tracking features, we need to identify strong features worth tracking. The Shi-Tomasi corner detector (an improvement over Harris) is commonly used with KLT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92e25bf-a1a1-4668-bc74-e3e3c23b1127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d08e023-40b4-421b-8598-e60bc505e388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "goodFeaturesToTrack(image, maxCorners, qualityLevel, minDistance[, corners[, mask[, blockSize[, useHarrisDetector[, k]]]]]) -> corners\n",
       ".   @brief Determines strong corners on an image.\n",
       ".   \n",
       ".   The function finds the most prominent corners in the image or in the specified image region, as\n",
       ".   described in @cite Shi94\n",
       ".   \n",
       ".   -   Function calculates the corner quality measure at every source image pixel using the\n",
       ".       #cornerMinEigenVal or #cornerHarris .\n",
       ".   -   Function performs a non-maximum suppression (the local maximums in *3 x 3* neighborhood are\n",
       ".       retained).\n",
       ".   -   The corners with the minimal eigenvalue less than\n",
       ".       \\f$\\texttt{qualityLevel} \\cdot \\max_{x,y} qualityMeasureMap(x,y)\\f$ are rejected.\n",
       ".   -   The remaining corners are sorted by the quality measure in the descending order.\n",
       ".   -   Function throws away each corner for which there is a stronger corner at a distance less than\n",
       ".       maxDistance.\n",
       ".   \n",
       ".   The function can be used to initialize a point-based tracker of an object.\n",
       ".   \n",
       ".   @note If the function is called with different values A and B of the parameter qualityLevel , and\n",
       ".   A \\> B, the vector of returned corners with qualityLevel=A will be the prefix of the output vector\n",
       ".   with qualityLevel=B .\n",
       ".   \n",
       ".   @param image Input 8-bit or floating-point 32-bit, single-channel image.\n",
       ".   @param corners Output vector of detected corners.\n",
       ".   @param maxCorners Maximum number of corners to return. If there are more corners than are found,\n",
       ".   the strongest of them is returned. `maxCorners <= 0` implies that no limit on the maximum is set\n",
       ".   and all detected corners are returned.\n",
       ".   @param qualityLevel Parameter characterizing the minimal accepted quality of image corners. The\n",
       ".   parameter value is multiplied by the best corner quality measure, which is the minimal eigenvalue\n",
       ".   (see #cornerMinEigenVal ) or the Harris function response (see #cornerHarris ). The corners with the\n",
       ".   quality measure less than the product are rejected. For example, if the best corner has the\n",
       ".   quality measure = 1500, and the qualityLevel=0.01 , then all the corners with the quality measure\n",
       ".   less than 15 are rejected.\n",
       ".   @param minDistance Minimum possible Euclidean distance between the returned corners.\n",
       ".   @param mask Optional region of interest. If the image is not empty (it needs to have the type\n",
       ".   CV_8UC1 and the same size as image ), it specifies the region in which the corners are detected.\n",
       ".   @param blockSize Size of an average block for computing a derivative covariation matrix over each\n",
       ".   pixel neighborhood. See cornerEigenValsAndVecs .\n",
       ".   @param useHarrisDetector Parameter indicating whether to use a Harris detector (see #cornerHarris)\n",
       ".   or #cornerMinEigenVal.\n",
       ".   @param k Free parameter of the Harris detector.\n",
       ".   \n",
       ".   @sa  cornerMinEigenVal, cornerHarris, calcOpticalFlowPyrLK, estimateRigidTransform,\n",
       "\n",
       "\n",
       "\n",
       "goodFeaturesToTrack(image, maxCorners, qualityLevel, minDistance, mask, blockSize, gradientSize[, corners[, useHarrisDetector[, k]]]) -> corners\n",
       ".\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv2.goodFeaturesToTrack?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f7400-21ff-4cb9-b584-ab4105794a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c795cb74-3711-4526-97b5-0ff7c00918e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read first frame (this may not ideal for your application)\n",
    "\n",
    "# Never picks new features to detect. Ideally, you do this every n seconds.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture('road_car_view.mp4')\n",
    "ret, first_frame = cap.read()\n",
    "gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect good features to track\n",
    "feature_params = dict(\n",
    "    maxCorners=100,\n",
    "    qualityLevel=0.3,\n",
    "    minDistance=7,\n",
    "    blockSize=7\n",
    ")\n",
    "p0 = cv2.goodFeaturesToTrack(gray, mask=None, **feature_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990206ec-5887-4cb4-8700-adc60fa93f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37288c1b-02e2-4cf3-85e9-368c6ef6447b",
   "metadata": {},
   "source": [
    "### 3.3 Implementing KLT Feature Tracking\n",
    "\n",
    "Now, let's implement the KLT tracker using OpenCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d196f409-af9d-48fe-b7b6-32da685077a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb122e8-8961-4974-8bf0-daaa65ff5610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5ab71c-995b-475c-a262-fa33971cf84c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d5998bc-5a90-4d27-bf8e-bd70d166fbfb",
   "metadata": {},
   "source": [
    "### 3.4 Building a Complete KLT Tracker with Trajectory Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc10ac4-cdb9-4252-be33-109f8efe5535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After initializing `p0`\n",
    "trajectories = [[] for _ in range(len(p0))]\n",
    "\n",
    "# Inside the while loop (after selecting good points)\n",
    "for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "    a, b = new.ravel()\n",
    "    if i < len(trajectories):\n",
    "        trajectories[i].append((a, b))\n",
    "\n",
    "# After video loop (plotting)\n",
    "for traj in trajectories:\n",
    "    traj = np.array(traj)\n",
    "    if len(traj) > 1:\n",
    "        plt.plot(traj[:, 0], traj[:, 1])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Feature Trajectories\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03f2f5e-0498-4058-a9eb-c54461012e66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50768b10-dc8d-4435-8bb1-7c02ed8e3617",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Interactive Components - Try It Yourself\n",
    "\n",
    "### 4.1 Experiment with KLT Parameters\n",
    "\n",
    "Modify the KLT parameters and observe how they affect tracking quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff283426-677e-4534-967c-0536c5cd5dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_klt(win_size=(15,15), max_level=2, quality=0.3, max_corners=100):\n",
    "    cap = cv2.VideoCapture('your_video.mp4')\n",
    "    ret, first_frame = cap.read()\n",
    "    gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "    p0 = cv2.goodFeaturesToTrack(gray, maxCorners=max_corners, qualityLevel=quality, minDistance=7, blockSize=7)\n",
    "    lk_params = dict(winSize=win_size, maxLevel=max_level,\n",
    "                     criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "    old_gray = gray.copy()\n",
    "    mask = np.zeros_like(first_frame)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "        good_new = p1[st == 1]\n",
    "        good_old = p0[st == 1]\n",
    "\n",
    "        for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "            a, b = new.ravel()\n",
    "            c, d = old.ravel()\n",
    "            mask = cv2.line(mask, (a, b), (c, d), (0, 255, 0), 2)\n",
    "            frame = cv2.circle(frame, (a, b), 5, (0, 0, 255), -1)\n",
    "        img = cv2.add(frame, mask)\n",
    "        cv2.imshow('KLT', img)\n",
    "        if cv2.waitKey(30) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "        old_gray = frame_gray.copy()\n",
    "        p0 = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0f8a3c9-4a2d-4043-916b-7101ec4b44c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and modify the parameters below\n",
    "lk_params = dict(\n",
    "    winSize=(15, 15),  # Try different sizes: (7,7), (21,21), etc.\n",
    "    maxLevel=2,        # Try different pyramid levels: 0, 3, 4\n",
    "    criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)  # Try different thresholds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a2b472-5e92-40f7-8759-755d24729d58",
   "metadata": {},
   "source": [
    "**Questions to explore:**\n",
    "1. What happens if you increase the window size? Does it handle faster motion better?\n",
    "2. What effect does the pyramid level have on tracking accuracy and computation time?\n",
    "3. How does the minimum eigenvalue threshold affect the number of tracked points?\n",
    "\n",
    "### 4.2 Investigate Feature Selection Parameters\n",
    "\n",
    "Experiment with feature detection parameters and observe the effects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c1e66b5-6e81-48a0-b507-c1a2f860a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different parameters\n",
    "corners = cv2.goodFeaturesToTrack(\n",
    "    gray, \n",
    "    maxCorners=100,       # Try: 50, 200, 500\n",
    "    qualityLevel=0.3,     # Try: 0.1, 0.5, 0.8\n",
    "    minDistance=7,        # Try: 5, 10, 20\n",
    "    blockSize=7           # Try: 3, 11, 15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b878356a-c20d-4c83-9adc-4e37b594b124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58444c32-54b5-4497-9813-ec357a843cfc",
   "metadata": {},
   "source": [
    "**Questions to explore:**\n",
    "1. How does `qualityLevel` affect the distribution of features?\n",
    "2. What's the effect of `minDistance` on closely spaced features?\n",
    "3. How does `blockSize` influence feature detection in textured vs. smooth regions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd1dcd-845a-473e-9a6e-feb175eeb135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90f70c2b-6687-4035-b079-416c45f8d478",
   "metadata": {},
   "source": [
    "## 5. Common Pitfalls and Troubleshooting\n",
    "\n",
    "| Issue | Cause | Solution |\n",
    "|-------|-------|----------|\n",
    "| **Lost Tracks** | Fast motion exceeding search window | Increase window size, use pyramid levels, consider frame interpolation |\n",
    "| **Drift** | Accumulation of small errors over time | Periodically reinitialize with detection, use template correction |\n",
    "| **Feature Clustering** | Similar appearance in multiple regions | Increase minimum distance between features, enforce spatial distribution |\n",
    "| **Background Features** | Features detected on background | Use foreground segmentation, focus detection on regions of interest |\n",
    "| **Illumination Changes** | Brightness constancy violation | Use normalized cross-correlation, consider illumination invariant features |\n",
    "| **Occlusion** | Objects temporarily hidden | Implement prediction models, handle reappearances with robust matching |\n",
    "| **Scale Changes** | Objects changing size | Use scale-adaptive windows, re-detect features periodically |\n",
    "| **Feature Selection** | Poor initial features | Increase quality threshold, use better feature detectors (SIFT/SURF/ORB) |\n",
    "| **Out-of-plane Rotation** | 3D rotation changing appearance | Use affine tracking model, consider 3D-aware tracking |\n",
    "| **ID Switching** | Similar objects crossing paths | Incorporate appearance models, use motion prediction, apply global optimization |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee527d9-017b-43cf-a732-7ace15a46e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba7f61c7-8b53-49ba-be59-300d15516bce",
   "metadata": {},
   "source": [
    "## 6. Practical Applications\n",
    "\n",
    "Feature and object tracking technologies have numerous real-world applications:\n",
    "\n",
    "1. **Video Surveillance**: Tracking people and vehicles across camera networks\n",
    "2. **Autonomous Vehicles**: Following other traffic participants and predicting their trajectories\n",
    "3. **Augmented Reality**: Stabilizing virtual content on real-world objects\n",
    "4. **Sports Analytics**: Tracking players and ball movement for performance analysis\n",
    "5. **Medical Imaging**: Following anatomical structures in ultrasound or microscopy\n",
    "6. **Human-Computer Interaction**: Gesture recognition and motion-based interfaces\n",
    "7. **Traffic Monitoring**: Measuring vehicle flow and detecting congestion\n",
    "8. **Robotics**: Visual servoing and environment navigation\n",
    "9. **Film Production**: Camera motion tracking for special effects\n",
    "10. **Wildlife Research**: Monitoring animal behavior without invasive tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b7d548-0875-4b37-9e87-b0249ec7c0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b430404-c3f2-482d-9335-d48e31c8ddde",
   "metadata": {},
   "source": [
    "## 7. Comparison of Tracking Methods\n",
    "\n",
    "| Method | Pros | Cons | Best Used For |\n",
    "|--------|------|------|---------------|\n",
    "| **KLT Tracker** | Fast, efficient for small motions, tracks arbitrary points | Drift over time, sensitive to appearance changes, doesn't handle occlusion well | Short-term tracking, simple motion, feature-rich objects |\n",
    "| **Mean-Shift/CAMShift** | Handles partial occlusions, adaptive to scale changes, histogram-based | Requires distinct color distribution, sensitive to lighting changes | Tracking objects with distinctive color profiles |\n",
    "| **Correlation Filters (KCF, MOSSE)** | Fast computation, handles appearance changes, discriminative model | Limited handling of scale changes, boundary effects | Real-time applications, moderately changing appearances |\n",
    "| **Particle Filters** | Robust to non-linear motion, handles multi-modal distributions | Computationally intensive, requires careful tuning | Complex, unpredictable motion patterns |\n",
    "| **Deep Learning Based (SiamFC, SiamRPN)** | Robust to appearance changes, occlusion handling, semantic understanding | Training data requirements, computational cost, may overfit to training domains | Long-term tracking, challenging environments |\n",
    "| **SORT/DeepSORT** | Handles multiple objects, manages identity, birth/death of tracks | Association challenges in crowded scenes, relies on good detections | Multi-object scenarios, surveillance, crowd analysis |\n",
    "| **IoU Tracker** | Simple, fast, effective for high framerate videos | Assumes spatial overlap, sensitive to detector failures | Dense frame scenarios with slow motion |\n",
    "| **3D Model-Based** | Very accurate, handles occlusion and viewpoint changes | Requires 3D models, computationally expensive | Industrial applications, AR/VR, precise tracking needs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968bdac5-760e-4441-93cb-e6d944b83c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7986ff3-6e51-4745-a94a-de7f6db12c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e837dd29-41c1-4807-b813-5afd51e5a721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
